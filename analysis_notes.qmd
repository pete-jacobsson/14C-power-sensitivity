---
title: "14C Power/Sensitivity notes"
author: "Piotr Jacobsson"
format: html
---

```{r, include=FALSE}
library(tidyverse)
library(broom)
library(jtools)
library(rstan)
library(rstanarm)

```


This document contains the research notes from the analysis of the simulations conducted using R and OxCal. This is a rough notebook that begins at the point when the project was re-initiated in June 2023 until completion

##Part 1: make sense of what is here and order things up
The first thing that I spotted was that much of the data files were sitting right in the main folder. So this will get ordered up - but I'll leave the Markdown files describing everything on top (I might need them lets see if past me had any brains worth of brain :P).

Next will be working out what the individual simulations are and what they cover - as well as taking some time to play with the Shiny App that I built to support the EDA. 
The thing looks much better now.

Lets have a look at the list of simulations we have.

```{r}
list_of_sims <- read_csv("sims_list.csv")
list_of_sims
```
There are a few things I could do here. I could start with the single calibrations and take it from there. Not sure what this would contribute though and I'd likely get bored. Also, there is the question of whether I want to keep working with test groups... Still could be fun refreshing what matters and chacking whether the calibration curve has anything to say about accuracy here.

For the wiggle-match dates, there are a few things that could be done. The first is to run models for the steep section and for the plateau and compare results. The interesting thing to do here would maybe be to use Stan while I am at it? The data sets are not that large and even if they turn out to be, I can truncate them. Yes, this could work.
Of course then there is the 300k project. It covers 1500 years, so the simulation density is 200 sims per year. Here I remember I wanted to go decade by decade, possibly with different regression models and start comparing. So we have options here for accuracy.
I might leave Sequences out at first pass.

Likewise I think it might be best to focus on accuracy early on - precision can come second. 

Oh and the Shiny will need a batch file. 

Had a quick look at Shiny. Next instalment should likely be some EDA with what I've got in the app and a decision if I want to maybe begin by completing that app.



# Single calibrations

## Part 2: Exploring the single radiocarbon dates with Shiny, and some prelim models

After some reflection I decided to start off with doing some model-based exploration of single calibrations. Using the Shiny App I was able to make some observations that helped.

1. The nominal accuracy was more or less OK for offsets up to about 15 - 25 14C years.
2. Greater measurement errors seem to mean more accurate dates
3. Measurements from different parts of the calibration curve may have different susceptibility to offsets. This seems to work different for positive and negative offsets.
4. Depending on the location on the calibration curve it may be that different offset directions have different effects.

Knowing this, the next step is to explore if any of these initial observations make it through exploratory modelling. The goal is to check is different parts of the calibration curve are comparable when it comes to effects of offset magnitudes and their interactions with measurement error magnitude.




```{r}
### Load the data
single_cals <- read_csv("simulation_results/singles_011_results.csv")
```


```{r}
###Seperate pos from neg offsets and group things by cal curve 
single_cals_modelled <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 50)
  ) %>%
  select(-target_year)


single_cals_modelled <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 50)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_modelled) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()


### Now lets specify a model to test if things work
# offset_only_acc68 <- function(singles_data) {
#   glm(accuracy_68 ~ offset_magnitude, data = singles_data, family = binomial(link = 'logit'))
# }

### After which we apply the model across the DF and check if everythign works as it should.


# single_cals_modelled <- single_cals_modelled %>%
#   mutate(offset_only_acc68 = map(data, offset_only_acc68))
# 
# summary(single_cals_modelled$offset_only_acc68[[1]])
##Comment out after testing to avoid clashes

## Great! Now build remaining models and apply to the DF. 

```

## Part 3 Fitting multiple logistic regressions
In this step I want to fit multiple logistic regressions to single calibrations I have the data frame ready, next step is to write the regression functions and apply them to the DF. 

```{r}
### Build functions to use with map
## Could have tried in-line but it would be very confusing with the number of parameters floating about. 

offset_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude, data = singles_data, 
      family = binomial)
}

sigma_only_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


offset_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

sigma_only_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + 
        measurement_error * offset_magnitude, 
      data = singles_data, family = binomial(link = 'logit'))
}


```

```{r}
### Now use map to apply the models across the data
single_cals_modelled <- single_cals_modelled %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )

```


## Part 4: Unpack the ensted lists and re-organize the DFs

Right now I have one DF with a lot of nested models. These need unpacked and re-organized into a new DF if they are to be visualizable (which is our objective after all!).

Lets try to loop through the unpacking and then create two DFs - one for parameters and one for model quality indicators. That would also be a good point to save the DFs into csv files (so we don't need to keep re-building them each time we re-visit the data).

```{r}
model_names <- colnames(single_cals_modelled)[4:11]

single_cals_log_results <- data.frame()
single_cals_log_diagnostics <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled %>%
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results <- rbind(single_cals_log_results, temp_results)
  single_cals_log_diagnostics <- rbind(single_cals_log_diagnostics, 
                                       temp_diagnostics)

}


write_csv(single_cals_log_results, 
          "model_results/single_cals_log_results.csv")
write_csv(single_cals_log_diagnostics, 
          "model_results/single_cals_log_diagnostics.csv")

```



## Part 5: Start plotting what all the results mean :D

There are two things we are interested in here. First is, how do different parameters for different models vary with time. Second, what are the model quality indicators. I'll start by looking at the latter, more specifically at AIC and BIC values.

```{r}
single_cals_log_diagnostics <- read_csv("model_results/single_cals_log_diagnostics.csv")

```

```{r}

## Lets make some plots
single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos, labeller = labeller(c("Pos offset", "Neg offset"))) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")

single_cals_log_diagnostics %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = AIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw()+
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


single_cals_log_diagnostics %>%
   mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
  ) %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = target_year, y = BIC)) +
  geom_line(aes(color = model)) +
  facet_wrap(~offset_pos) +
  theme_bw() +
  scale_color_manual(values = c("darkblue", "blue", "steelblue", "gray")) +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )) +
  labs(x = "Cal yrs BP")


  
```

There are a few things to note here. 
First, as expected the Sigma-only model performs worst of the whole group.

Second, the remaining models are close to each other in performance, with the interaction model performing marginally better with respect to AIC and the offset only model performaing little better with respect to BIC

Third, the model AIC and BIC varies with the set of target dates - suggesting that the performance of logistic regression varies depending on the shape of the calibration curve at a given segment of time. Lets explore if we can find a pattern by adding in calibration curve information and drawing some scatter plots.

```{r}
## Can't just bin cal curve. Does not match well
## First get cal curve and min/max for the actual bins

cal_curve <- read_csv("intcal_20_interpolated.csv")

binned_sim_yrs <- single_cals %>%
  mutate(bin_yrs = ntile(target_year, 50)) %>%
  group_by(bin_yrs) %>%
  summarize(
    bin_end = max(target_year),
    target_year = min(target_year)
    
  )

## Now loop through each bin, and get BP sd for that group of years - a proxy for how variable the cal curve was (less = more plateau)
bp_sd <- c()

for (i in 1:nrow(binned_sim_yrs)) {
  binned_sd <- cal_curve %>%
    filter(CalBP >= binned_sim_yrs$target_year[i] & 
             CalBP <= binned_sim_yrs$bin_end[i]) %>%
    summarize(bin_sd = sd(BP)) %>%
    pull(bin_sd)
  
  bp_sd <- c(bp_sd, binned_sd)
}

binned_sim_yrs <- cbind(binned_sim_yrs, bp_sd) %>%
  select(target_year, bp_sd)

###Errors here. Re-work to ensure match!


inner_join(single_cals_log_diagnostics, binned_sim_yrs) %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = bp_sd, y = AIC)) +
  geom_point(aes(colour = model)) +
  facet_wrap(~model) +
  theme_bw() +
  scale_color_manual(values = c("grey10", "grey30", "grey50", "darkviolet")) +
  theme(panel.grid.minor = element_blank())
  
```
So there is no pattern here at first sight. What we will want to do in this case is to re-do the simulation to confirm that the AIC shape remains stable. If so, we can take another pass looking at whether we can define a good metric for cal curve variability (if we are thus inclined).



Fourth - and that's the interesting part - the AIC/BIC behavior differs between the models for the 68% HPD areas and 95% HPD areas. For the former, while variable, we see no overall trend and the sigma_only model has higher AIC/BIC. The 95% HPD models have a clear downward trend and a much smaller gap between the AIC/BICs for different model types. The small gap implies is that, for the 95% models, measurement error alone is almost as good as knowing the offset magnitude at calling whether the result is off-target or not. This is something that will require some more digging into. Second, all models become better as we go back in time for the 95%. As the main difference is in the precision of the calibration curve, this implies that curve ucnertainty may play a role in how well a logistic regression will predict model fit. 


With these ideas now in mind, lets have a look at how different points in time compare. At first, we shall use the interaction model to investigate (it has best AIC and also makes most sense logically - but lets see the outcomes first).

```{r}
single_cals_log_results <- read_csv("model_results/single_cals_log_results.csv")
```

First lets plot the intercept

```{r}
single_cals_log_results %>%
  filter(str_detect(model, "interact") & str_detect(term, "Intercept")) %>%
  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & 
             str_detect(model, "95"))) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_hline(yintercept = 0) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = offset_pos), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = offset_pos), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    )
```




Now lets plot the remaining parameters.
```{r}
single_cals_log_results %>%
  filter(str_detect(model, "interact") & !str_detect(term, "Intercept")) %>%
  filter(!(target_year %in% c(7019, 7751, 9076, 11091) & 
             str_detect(model, "95"))) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = term), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = term), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model)) +
  ylim(c(-2, 2)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Parameter", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Parameter")
```

Some quick notes.

Big blobby bits in the 95% model. Lets check what they are.

```{r}
single_cals_log_results %>%
  filter(std.error >0.75 & !str_detect(term, "tercept"))
```

There are a few observations at hand here. 

_First_, there were some extreme values for the models covering the 95% HPD range. These have been removed from the figure, but likely correspond to the models that did not want to converge (see warnings up top!).

_Second_, we can see that the 95% models have much greater uncertainties than the 68% models - in other words they are not as good precise as the estimates from the 68% HPD ranges. This does make sense - the 95% ranges will have a much greater number of on-terget estimates than the 68% ranges - therefore being more difficult to treat with logistic regression. 

_Third_, we see some variation in regression parameters through time. To decide whether it is meaningful in the context of the current sample, we need to check if the estimates from year to year are different enough from one another. We'll build a Z-test.
```{r}
## this will get a little messy. In order for the thing to work efficiently, it will be easier to pivot the table wider
single_cals_log_results_wider_pos <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))

single_cals_log_results_wider_neg <- single_cals_log_results %>%
  filter(str_detect(model, "interact") & !offset_pos) %>%
  select(!c(statistic, p.value)) %>%
  pivot_wider(names_from = term, values_from = c(estimate, std.error))
```



```{r}
#### I'm leaving the above for posterity to see the error of my ways.... this approach is too ugly. What we really need is two functions that will produce the vectors required. An estimate difference calculator and a chi squared estimator. What they'll need to do is to take on a vector of estimates, a vector of standard deviations and return a vector of the desired type of results.



estimate_running_diff <- function(estimates, stdevs) {
  ###Note assumptions: vectors longer than two, and equal length!
  ###If ever planned to publish the function would need a check and a warning for input lengths!
  
  difs <- c() ## our results will go here
  
  for (i in 2 : length(estimates)) {
    estimate_curr <- estimates[i] # A flourish - makes it easier for the likes of me to read!
    estimate_prev <- estimates[i-1]
    stdev_curr <- stdevs[i]
    stdev_prev <- stdevs[i-1]
    
    estimate_diff <- (estimate_curr - estimate_prev) /
                     (sqrt(stdev_curr^2 + stdev_prev^2))
    
    difs <- c(difs, estimate_diff)
  }
  
  difs
  
}

estimate_running_diff(single_cals_log_results_wider_neg$estimate_offset_magnitude,
                      single_cals_log_results_wider_neg$std.error_offset_magnitude)

## Now build the df. Yes, there is a smarter way...
z_test_comparisons <- data.frame(
  
  target_year = single_cals_log_results_wider_pos$target_year[2:100],
  pos_offset = estimate_running_diff(
    single_cals_log_results_wider_pos$estimate_offset_magnitude,
    single_cals_log_results_wider_pos$std.error_offset_magnitude),
  pos_error = estimate_running_diff(
    single_cals_log_results_wider_pos$estimate_measurement_error,
    single_cals_log_results_wider_pos$std.error_measurement_error
  ),
  pos_intercept = estimate_running_diff(
    single_cals_log_results_wider_pos$`estimate_(Intercept)`,
    single_cals_log_results_wider_pos$`std.error_(Intercept)`
  ),
  pos_interaction = estimate_running_diff(
    single_cals_log_results_wider_pos$`estimate_measurement_error:offset_magnitude`,
    single_cals_log_results_wider_pos$`std.error_measurement_error:offset_magnitude`
  ),
  neg_offset = estimate_running_diff(
    single_cals_log_results_wider_neg$estimate_offset_magnitude,
    single_cals_log_results_wider_neg$std.error_offset_magnitude),
  neg_error = estimate_running_diff(
    single_cals_log_results_wider_neg$estimate_measurement_error,
    single_cals_log_results_wider_neg$std.error_measurement_error
  ),
  neg_intercept = estimate_running_diff(
    single_cals_log_results_wider_neg$`estimate_(Intercept)`,
    single_cals_log_results_wider_neg$`std.error_(Intercept)`
  ),
  neg_interaction = estimate_running_diff(
    single_cals_log_results_wider_neg$`estimate_measurement_error:offset_magnitude`,
    single_cals_log_results_wider_neg$`std.error_measurement_error:offset_magnitude`
  )
  
) %>%
  pivot_longer(!target_year, names_to = "parameter", values_to = "z_value") %>%
  mutate(
    offset_direction = if_else(str_detect(parameter, "pos"), "pos", "neg"),
    parameter = str_remove(parameter, "pos_|neg_")
  )


```

```{r}
z_test_comparisons %>%
  ggplot(aes(x = target_year, y = z_value)) +
  geom_hline(yintercept = c(-2, 2), size = 0.5, linetype = "dashed") +
  geom_line(aes(color = offset_direction)) +
  facet_wrap(~parameter) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
   scale_color_manual( values = c("steelblue", "darkblue"))
```
On the whole, not much to talk about, a few instances of going over 2z, but this is not decisive given the number of data points we're talking about. Now, what is perhaps a little more interesting is that the Z values have greater swings between bins in most recent years, but more subtle ones the further back we go in time. 


_Fourth_, we see that the interaction parameter is very close to zero. This implies we could get similar results by using a simpler model. I find this very counter-intuitive - I suspect that the term for impacts of measurement precision (measurement_error) might start turning the other way, i.e. right now the measurement error estimates tend to imply that increasing the measurement error deteriorates the accuracy of the calibrated date models. Lets see if this will remain the case under different model specifications. Lets also check how this will affect the offset parameter under the different models. As the effects are greater under the 68% HPD models, this is where I'll focus attention.

```{r}
single_cals_log_results %>%
  filter(!str_detect(model, "95") & 
           !str_detect(model, "sigma_only") & 
           !str_detect(term, "Intercept") & 
           !str_detect(term, "error:offset")) %>%  ## This removes extreme values of the 95% model in the visualizations
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset")
    )%>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_hline(yintercept = 0, size = 0.5) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = model), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = model)) +
  facet_grid(cols = vars(offset_pos), rows = vars(term)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    strip.background = element_rect(
     color="black", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Model", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Model")
```

OK, suspicion confirmed. What we see here is that the models without the interaction terms are on the one hand much more precise as far as their estimates go, but on the other hand, the parameter for the measurement error does indeed flip sigm - in the interaction models its mean is negative (implies greater scatter of estimates leads to greater chance of measurement being off target), while in the non-interaction models it is positive - implying that greater measurement uncertainty leads to greater precision. Now, we know that in the case of zero offset from the mean of the calibration curve, a super-precise determination would almost always fit within the uncertainty of the calibration curve - and therefore return more accurate results. As such, due to underpinning maths, the interaction model is a better description of reality even if the interaction term itself is centered around zero and the overall parameter estimate uncertainties are far higher.



Now, what all this did not give us is reproduction of the trends we've seen in the EDA histograms. What I'd be keen to do is to try again with wider bins. There will be a big ole' copy paste comin' this way, I'm afraid. 


## Part 6 Lets check a different bin size
The outcomes above are deeply suspicious given a clear difference that came out in the EDA - there should be some difference in something between models for data in the more recent bits of the curve and those for more distant times. To this end, lets re-do the last few hundred lines, but at just ten bins (rather than 50).]

```{r}
### Load the data

single_cals <- read_csv("simulation_results/singles_011_results.csv")

###Seperate pos from neg offsets and group things by cal curve 
single_cals_binned10 <- single_cals %>%
  mutate (
    offset_pos = if_else(offset_magnitude > 0, TRUE, FALSE),
    offset_magnitude = if_else(offset_pos, 
                               offset_magnitude, offset_magnitude * -1),
    # Changing negs on offset magnitude to have a consistent direction at downstream visualization.
    binned_targets = ntile(target_year, 10)
  ) %>%
  select(-target_year)


single_cals_binned10 <- single_cals %>%
  mutate(binned_targets = ntile(target_year, 10)) %>%
  group_by(binned_targets) %>%
  summarize( #Easy way to get years for the indiv bins, without too muchj headach
    target_year = min(target_year)
    ) %>%
  inner_join(single_cals_binned10) %>% #Join the binned DF back in. No, I don't like this either!
  group_by(offset_pos, target_year) %>%
  nest()

```

```{r}
### Now use map to apply the models across the data structured into only 10 bins
single_cals_modelled_bin10 <- single_cals_binned10 %>%
  mutate(
    offset_only_acc68 = map(data, offset_only_acc68),
    sigma_only_acc68 = map(data, sigma_only_acc68),
    offset_sigma_acc68 = map(data, offset_sigma_acc68),
    offset_sigma_interact_acc68 = map(data, offset_sigma_interact_acc68),
    offset_only_acc95 = map(data, offset_only_acc95),
    sigma_only_acc95 = map(data, sigma_only_acc95),
    offset_sigma_acc95 = map(data, offset_sigma_acc95),
    offset_sigma_interact_acc95 = map(data, offset_sigma_interact_acc95)
  )

```

```{r}
model_names <- colnames(single_cals_modelled_bin10)[4:11]

single_cals_log_results_bin10 <- data.frame()
single_cals_log_diagnostics_bin10 <- data.frame()

for (model in model_names) {
  ## This will get a little experimental - we are trying to create a big old table!
  ## First, we thin down the DF to the model of interest
  temp_results <- single_cals_modelled_bin10 %>%
    select(1, 2, all_of(model)) %>% #All of used to address deprecation
    rename(glm_list = 3) %>% #This rename allows map to work correctly
    mutate(
      logistic_results = map(glm_list, tidy), # Do the map and also take note of which model
      model = model
    ) %>%
    select(-glm_list) %>% # Gets rid of the actual models (necessary for saving as csv)
    unnest(logistic_results)# Unnests results
  
  temp_diagnostics <- single_cals_modelled_bin10 %>%
    select(1, 2, all_of(model)) %>%
    rename(glm_list = 3) %>%
    mutate(
      logistic_diagnostics = map(glm_list, glance),
      model = model
    ) %>%
    select(-glm_list) %>%
    unnest(logistic_diagnostics)
  
  single_cals_log_results_bin10 <- rbind(single_cals_log_results_bin10, temp_results)
  single_cals_log_diagnostics_bin10 <- rbind(single_cals_log_diagnostics_bin10, 
                                         temp_diagnostics)

}


write_csv(single_cals_log_results_bin10, 
          "model_results/single_cals_log_results_bin10.csv")
write_csv(single_cals_log_diagnostics_bin10, 
          "model_results/single_cals_log_diagnostics_bin10.csv")

```


Now lets re-load the results and re-plot the parameters to see if anything changes. 
```{r}
single_cals_log_results_bin10 <- read_csv(
  "model_results/single_cals_log_results_bin10.csv")
```

First lets plot the intercept

```{r}
single_cals_log_results_bin10 %>%
  filter(str_detect(model, "interact") & str_detect(term, "Intercept")) %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = model), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = model), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(colour = "grey80", size = 0.1),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(values = c("steelblue", "darkblue")) +
  scale_color_manual(values = c("steelblue", "darkblue"))
```

So what we can see here is that, while the estimates are still effectively similar, the intercept does seem to go up the further back we go in time. 

```{r}
single_cals_log_results_bin10 %>%
  filter(str_detect(model, "interact") & !str_detect(term, "Intercept")) %>%
  mutate(
    offset_pos = if_else(offset_pos, "Pos offset", "Neg offset"),
    model = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
  ) %>%
  ggplot(aes(x = target_year, y = estimate)) +
  geom_ribbon(aes(ymin = estimate - std.error * 2, 
                  ymax = estimate + std.error * 2,
                  fill = term), alpha = 0.2, color = "grey75") +
  geom_line(aes(color = term), linetype = "dotted") +
  facet_grid(cols = vars(offset_pos), rows = vars(model)) +
  #ylim(c(-0.25, 0.25)) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(colour = "grey80", size = 0.1),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    ) +
  scale_fill_manual(name = "Parameter", values = c("steelblue", "grey50", "darkblue")) +
  scale_color_discrete(name = "Parameter")
```
Everything here is kinda flat. So the difference seems to live in the intercept - which makes sense. At this point variation due calibration curve can be ignored (we just used bins that would aggregate a lot of the calibration curve). This implies that the differences that we can detect with this set of simulations are not due to the fine shape of the curve, but maybe rather due to the magnitude of the unceartainty about the calibration curve estimate itself. 


## Part 7 Exploring models that include calibration curve uncertainty

To check whether this is the case, lets take a different approach and see whether uncertainty about the calibration curve may have some kind of a role to play. Now, there exists a risk that this might not be a good estimate at points such as calibration plateaux or steep sections, so we'll need to do two bits of prep:

1. Join the data to calibration curve uncertainty at given target date
2. Set chunks of the simulated data aside.

```{r}
single_cals_w_curve_uncert <- read_csv("intcal_20_interpolated.csv") %>%
  select(CalBP, Error) %>%
  rename(target_year = CalBP, curve_uncert = Error) %>%
  inner_join(single_cals) %>%
  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))

write_csv(single_cals_w_curve_uncert, "single_cals_w_curve_uncert.csv")
```

```{r}
## Take a quick look at how curve uncertainty goes with accuracy

single_cals_w_curve_uncert %>%
  mutate(curve_uncert = round(curve_uncert)) %>%
  group_by(curve_uncert) %>%
  summarize(
    `68.2% HPD` = mean(accuracy_68),
    `95.4% HPD` = mean(accuracy_95)
  ) %>%
  pivot_longer(!curve_uncert, values_to = "accuracy", names_to = "hpd_area") %>%
  ggplot(aes(x = curve_uncert, y = accuracy)) +
  geom_bar(stat = 'identity', fill = "steelblue") +
  facet_wrap(~hpd_area)+
  labs(
    x = "Calibration curve uncertainty",
    y = "Ratio accurate"
  ) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel")
    )
  
```

OK, we can see that there is a relationship between calibration curve uncertainty and accuracy of the different modelled HPD areas.

Next step is to build a few models and compare their performance. Lets recall that we want to keep some of the data out, to ensure that we can use it to test the models at the end of the process. 
To do this, I chose the following periods:
- All data from 2400 to 2700 cal BP (a calibration plateau)
- All data from 6500 to 6800 cal BP (a "straight" section of the curve)
- All data from 9400 to 9600 cal BP

```{r}
###Step 1. Extract bits that we want to use for model validation
single_cals_curve_uncert_val <- single_cals_w_curve_uncert %>%
  filter((target_year >= 2400 & target_year <= 2700) | 
         (target_year >= 6500 & target_year <= 6800) | 
         (target_year >= 9400 & target_year <= 9500))

single_cals_curve_uncert_mod <- single_cals_w_curve_uncert %>%
  filter(!((target_year >= 2400 & target_year <= 2700) | 
           (target_year >= 6500 & target_year <= 6800) | 
           (target_year >= 9400 & target_year <= 9500)))

## Now these get put into csv files for use later:

write_csv(single_cals_curve_uncert_val, "single_cals_curve_uncert_val.csv")
write_csv(single_cals_curve_uncert_mod, "single_cals_curve_uncert_mod.csv")
```

```{r}
### Now load up the model data again, nest it and build the model functions
single_cals_curve_uncert_mod <- read_csv("single_cals_curve_uncert_mod.csv") %>%
  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))
```

```{r}
single_cals_w_curve_uncert <- read_csv("single_cals_w_curve_uncert.csv")

##Nesting
single_cals_curve_uncert_nest <- single_cals_w_curve_uncert %>%
  mutate(is_pos = if_else(offset_magnitude >= 0, TRUE, FALSE)) %>%
  select(curve_uncert, measurement_error, offset_magnitude, accuracy_68,
         accuracy_95, is_pos) %>%
  group_by(is_pos) %>%
  nest()
```

```{r}
## Write model functions for 68% HPDs

offset_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + curve_uncert, data = singles_data, 
      family = binomial)
  }

sigma_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + curve_uncert, data = singles_data, 
      family = binomial(link = 'logit'))
  }

offset_sigma_curve_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_curve_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + curve_uncert + 
        offset_magnitude * curve_uncert, 
      data = singles_data, 
      family = binomial)
  }
  
sigma_curve_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, 
      family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_sc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_oc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_oc_sc_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_oc_sc_ocs_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert +
        offset_magnitude * measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_os_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * measurement_error, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_interact_os_cs_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * measurement_error + curve_uncert * measurement_error, 
      data = singles_data, family = binomial(link = 'logit'))
  }

offset_sigma_curve_comb_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + sigma_curve_uncert,
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_comb_interact_acc68 <- function(singles_data) {
  glm(accuracy_68 ~ offset_magnitude + sigma_curve_uncert +
        offset_magnitude * sigma_curve_uncert,
      data = singles_data, family = binomial(link = 'logit'))
  }

```

```{r}
## And now have those set up for the 95% HPD ranges -- yes I could have pivoted longer. I know!

offset_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + curve_uncert, data = singles_data, 
      family = binomial)
}

sigma_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + curve_uncert, data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_curve_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + curve_uncert + 
        offset_magnitude * curve_uncert, 
      data = singles_data, 
      family = binomial)
}

sigma_curve_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, 
      family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_sc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_oc_sc_ocs_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * curve_uncert + measurement_error * curve_uncert +
        offset_magnitude * measurement_error * curve_uncert, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_os_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * measurement_error, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_interact_os_cs_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ measurement_error + offset_magnitude + curve_uncert +
        offset_magnitude * measurement_error + curve_uncert * measurement_error, 
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_comb_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + sigma_curve_uncert,
      data = singles_data, family = binomial(link = 'logit'))
}

offset_sigma_curve_comb_interact_acc95 <- function(singles_data) {
  glm(accuracy_95 ~ offset_magnitude + sigma_curve_uncert +
        offset_magnitude * sigma_curve_uncert,
      data = singles_data, family = binomial(link = 'logit'))
  }


```

### Model simulation lives here
```{r}
### The first two items are a later addition if further models need added

### Now load up the model data again, nest it and build the model functions
single_cals_w_curve_uncert <- read_csv("single_cals_w_curve_uncert.csv") %>%
  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))

##Nesting
single_cals_curve_uncert_nest <- single_cals_w_curve_uncert %>%
  mutate(is_pos = if_else(offset_magnitude >= 0, TRUE, FALSE)) %>%
  select(curve_uncert, measurement_error, offset_magnitude, accuracy_68,
         accuracy_95, is_pos, sigma_curve_uncert) %>%
  group_by(is_pos) %>%
  nest()


### Now lets run the models...
single_cals_curve_uncert_regr <- single_cals_curve_uncert_nest %>%
  mutate(
    offset_curve_acc68 = map(data, offset_curve_acc68),
    offset_curve_acc95 = map(data, offset_curve_acc95),
    offset_curve_interact_acc68 = map(data, offset_curve_interact_acc68),
    offset_curve_interact_acc95 = map(data, offset_curve_interact_acc95),
    offset_sigma_curve_acc68 = map(data, offset_sigma_curve_acc68),
    offset_sigma_curve_acc95 = map(data, offset_sigma_curve_acc95),
    offset_sigma_curve_interact_oc_acc68 = 
      map(data, offset_sigma_curve_interact_oc_acc68),
    offset_sigma_curve_interact_oc_acc95 = 
      map(data, offset_sigma_curve_interact_oc_acc95),
    offset_sigma_curve_interact_oc_sc_acc68 = 
      map(data, offset_sigma_curve_interact_oc_sc_acc68),
    offset_sigma_curve_interact_oc_sc_acc95 = 
      map(data, offset_sigma_curve_interact_oc_sc_acc95),
    offset_sigma_curve_interact_oc_sc_ocs_acc68 = 
      map(data, offset_sigma_curve_interact_oc_sc_ocs_acc68),
    offset_sigma_curve_interact_oc_sc_ocs_acc95 = 
      map(data, offset_sigma_curve_interact_oc_sc_ocs_acc95),
    offset_sigma_curve_interact_os_acc68 = 
      map(data, offset_sigma_curve_interact_os_acc68),
    offset_sigma_curve_interact_os_acc95 = 
      map(data, offset_sigma_curve_interact_os_acc95),
    offset_sigma_curve_interact_os_cs_acc68 = 
      map(data, offset_sigma_curve_interact_os_cs_acc68),
    offset_sigma_curve_interact_os_cs_acc95 = 
      map(data, offset_sigma_curve_interact_os_cs_acc95),
    offset_sigma_curve_comb_acc68 =
      map(data, offset_sigma_curve_comb_acc68),
    offset_sigma_curve_comb_acc95 =
      map(data, offset_sigma_curve_comb_acc95),
        offset_sigma_curve_comb_interact_acc68 =
      map(data, offset_sigma_curve_comb_interact_acc68),
    offset_sigma_curve_comb_interact_acc95 =
      map(data, offset_sigma_curve_comb_interact_acc95)
    
    )

```

```{r}
### Lets re-orgnize the table to make parameter extraction easier.
single_cals_curve_uncert_regr <- single_cals_curve_uncert_regr %>%
  select(-data) %>%
  pivot_longer(!is_pos, names_to = "model", values_to = "model_results")

saveRDS(single_cals_curve_uncert_regr, "single_cals_curve_uncert_regr.rds")
```

```{r}
single_cals_curve_uncert_regr <- readRDS("single_cals_curve_uncert_regr.rds")
```


## Part 8: Compare the models with cal curve uncertainty as a variable

I'll attack this the same way as before - first getting the AIC and the BIC and comparing. I guess I'll visualize them as a bar plot going from highest to lowest.

```{r}
### Extract the diagnostics
single_cals_curve_uncert_diag <- single_cals_curve_uncert_regr %>%
  mutate(model_diags = map(model_results, glance)) %>%
  select(-model_results) %>%
  unnest(cols = c(model_diags))
```

```{r}
### Now the visualization
single_cals_curve_uncert_diag %>%
  mutate(
    offset_direction = if_else(is_pos, "Positive", "Negative"),
    hpd_area = if_else(str_detect(model, "68"), "68.2% HPD Area", "95.4% HPD Area"),
    model = str_remove_all(model, "_acc\\d{2}")
  ) %>%
  ggplot(aes(x = model, y = AIC)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "grey40") +
  facet_grid(rows = vars(hpd_area), cols = vars(offset_direction), scales = "free") +
  labs(
    x = "Model",
    subtitle = "AIC of different models that include curve uncertainty"
  ) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    )

```

So the most complex models have the lowest AIC. In other words, those things seem to be doing something. Lets check BIC.

```{r}
single_cals_curve_uncert_diag %>%
  mutate(
    offset_direction = if_else(is_pos, "Positive", "Negative"),
    hpd_area = if_else(str_detect(model, "68"), "68.2% HPD Area", "95.4% HPD Area"),
    model = str_remove_all(model, "_acc\\d{2}")
  ) %>%
  ggplot(aes(x = model, y = BIC)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "grey40") +
  facet_grid(rows = vars(hpd_area), cols = vars(offset_direction), scales = "free") +
  labs(
    x = "Model",
    subtitle = "BIC of different models that include curve uncertainty"
  ) +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    )
```

Same. By AIC/BIC alone, most complex model is second best. *I went for it in the first place, because I have not thought enough to get the best model at that stage of writing - this came out after the debacle with params, as seen below :P*

```{r}
osc_interact_params <- single_cals_curve_uncert_regr %>%
  filter(str_detect(model, "oc_sc_ocs")) %>%
  mutate(model_results = map(model_results, tidy)) %>%
  unnest(cols = c(model_results))
```

```{r}
osc_interact_params %>%
  filter(str_detect(model, "68")) %>%
  ggplot(aes(x = is_pos, y = estimate)) +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_point(stat = "identity") +
  geom_linerange(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error)) +
  facet_wrap(~term, scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    ) +
  labs(
    subtitle = "68.2% HPD"
  )
```


```{r}
osc_interact_params %>%
  filter(str_detect(model, "95")) %>%
  ggplot(aes(x = is_pos, y = estimate)) +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_point(stat = "identity") +
  geom_linerange(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error)) +
  facet_wrap(~term, scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    ) +
  labs(
    subtitle = "95.4% HPD"
  )
```


Now that we have some results, lets start checking how well they fit the simulated data and, while we're at it, trying to understand what's happening. 

For the purpose of udnerstanding lets go back to the actual equation that's getting used. Lets begin by looking back at the equation behind logistic regression and what it means for us in this context:

$$
P(x) = \frac{1}{1 + e^{-(A_0 + A_1 x_1 + A_2 x_2 +...+A_n x_1x_2+...)}}
$$
Lets read this thing. The bigger the value in the brackets above the *e*, the closer the denominator to one and therefore the closer the probability of the result being correct (as the exponential races to zero). By contrast, the smaller the brackets in the denominator, the bigger the exponential result, and the whole thing gets closer to zero. Lets check what this gives us. With this we can start plotting the regression lines using the estimates:
```{r}

osc_interact_params_wider <- osc_interact_params %>%
  mutate(model = if_else(str_detect(model, "68"), "hpd_68", "hpd_95")) %>%
  select(model, is_pos, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate)

estimate_precision <- function(offset, error, curve_sigma, hpd) {
  osc_interact_params_wider %>%
    filter(is_pos == (offset >= 0), str_detect(model, hpd)) %>%
    mutate(
      exp_term = `(Intercept)` + offset_magnitude * offset + 
        measurement_error * error + 
        curve_uncert * curve_sigma +
        `offset_magnitude:curve_uncert` * offset * curve_sigma +
        `measurement_error:curve_uncert` * error * curve_sigma +
        `measurement_error:offset_magnitude` * error * offset_magnitude +
        `measurement_error:offset_magnitude:curve_uncert` * 
        error * offset_magnitude * curve_sigma,
      
      prob = 1 / (1 + exp(-exp_term))
    ) %>%
    pull(prob)
}

###Yes, it's overkill, but it's good to refresh things at a basic level
```

```{r}
estimate_precision(offset = -30, error = 16, curve_sigma = 20, hpd = "95")
  
curve(estimate_precision(x, error = 40, curve_sigma = 20, hpd = "95"))

```

OK, the estimates are screwed.


## Part 9 Try again, simpler model

The screwy estimates imply that the best-fitting, most complex model, even though satisfactory in terms of AIC/BIC, might not be best suited to research design simulation. Hence the need to settle for something simpler. Lets go back and cut out some of the interaction terms. While we will still need the term running between the measurement error and offset magnitude (so that the simulation model is consistent with the hard reality), everything else was adding nice nuance, but may not have been strictly necessary. This will require first building the relevant model and then checki

```{r}
##Get the results out

offset_curve_sigma_os_params <- readRDS("single_cals_curve_uncert_regr.rds") %>%
  filter(str_detect(model, "offset_sigma_curve_interact_os_acc")) %>%
  mutate(model_results = map(model_results, tidy)) %>%
  unnest(cols = c(model_results))
```

```{r}
  offset_curve_sigma_os_params %>%
  mutate(
    is_pos = if_else(is_pos, "pos_offset", "neg_offset"),
    hpd_area = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
    ) %>%
  ggplot(aes(x = is_pos, y = estimate)) +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_point(stat = "identity") +
  geom_linerange(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error)) +
  facet_grid(rows = vars(term), cols = vars(hpd_area), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    ) +
  labs(
    subtitle = "68.2% HPD"
  )
```

This will need to be made way prettier, but the parameters already look better. For starters there is a clear difference in the intercepts between the 95% HPDs and the 68% HPDs - which is what we'd expect. Interesting is the lower importance of curve uncertainty effects for the 68% models, as well as the much broader uncertainty of the measurement error in the 95% HPD calibrations. What I do find interestying, and promising is that the effect of the offset magnitude, according to this model, is similar across HPD areas, and that measurement uncertainties seem to have similar effect as well. OK, lets plot the outcomes against the simulated data to make sure this actually makes sense :D


## Part 10 Model visualization. Use a function.
Now that we have a model with promising looking parameters, lets visualize it. This is always a little bit of a headache once we get to multiple regression, especially as we want to plot a few visuals to see if our predictions actually match the data they originated from. To simplify the process, I'll use the jtools package. I went off to look for it, after having spent quite some time trying to reinvent it (:P).

```{r}

predict_model_results_osc <- function(offset_dir, hpd_area, model_name,
                                      variable, variable_range) {
  ##Function to simplify prediction generation
  ##Takes on offset_dir and hpd_area, which simplify the results table,
  ##Takes on variable name as string and variable range as a sequence
  ##Assumes offset dir, hpd_area, and variable name provided are all correct
  
  
  offset_curve_sigma_os <- readRDS("single_cals_curve_uncert_regr.rds") %>%
    filter(str_detect(model, model_name)) %>%
    mutate(is_pos = if_else(is_pos, "positive", "negative")) %>% ## Change type for filtering
    filter(str_detect(is_pos, offset_dir) & str_detect(model, hpd_area))
  
  predicted_results <- make_predictions(offset_curve_sigma_os$model_results[[1]],
                                        pred = variable,
                                        pred.values = variable_range)
  
  predicted_results
}

```


```{r}
## Function tests
predict_model_results_osc(offset_dir = "pos", hpd_area = "68",
                          model_name = "offset_curve_interact",
                          variable = "offset_magnitude", 
                          variable_range = seq(0, 50))

predict_model_results_osc(offset_dir = "neg", hpd_area = "95", 
                          model_name = "oc_sc_ocs",
                          variable = "curve_uncert", 
                          variable_range = seq(10, 35))

predict_model_results_osc(offset_dir = "pos", hpd_area = "95", 
                          model_name = "sc",
                          variable = "measurement_error", 
                          variable_range = seq(8, 32))

```


```{r}
plot_simulation_results_osc <- function(range_to_plot, hpd_area, variable,
                                        rounding, xlab) {
  ##This function plots the simulation results for the purposes of the results graphing
  
  single_cals_curve_uncert_mod <- read_csv("single_cals_w_curve_uncert.csv") %>%
  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))
  
  single_cals_curve_uncert_mod %>%
    rename(variable_to_plot = variable, hpd_to_plot = hpd_area) %>%
    filter(variable_to_plot >= range_to_plot[1] & 
             variable_to_plot <= range_to_plot) %>%
    mutate(
      variable_to_plot = plyr::round_any(variable_to_plot, rounding)
    ) %>%
    select(variable_to_plot, hpd_to_plot) %>%
    group_by(variable_to_plot) %>%
    summarize(
      ratio_accurate = mean(hpd_to_plot)
    ) %>%
    ggplot(aes(x = variable_to_plot, y = ratio_accurate)) +
    geom_bar(stat = 'identity', fill = "steelblue") +
    theme_bw() +
    theme(
      panel.grid.minor = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      axis.line = element_line(colour = "grey50", linewidth = 0.5),
      text = element_text(family = "Corbel")#,
      #axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
      ) +
    labs(
      y = "Ratio accurate",
      x = xlab
    )
  
  
}
```


```{r}
plot_simulation_results_osc(range_to_plot = c(10, 15), hpd_area = "accuracy_68",
                            variable = "curve_uncert", rounding = 5,
                            xlab = "Calibration curve uncertainty")
plot_simulation_results_osc(range_to_plot = c(24, 50), hpd_area = "accuracy_95",
                            variable = "measurement_error", rounding = 2.5,
                            xlab = "Measurement uncertainty")
plot_simulation_results_osc(range_to_plot = c(0, 50), hpd_area = "accuracy_68",
                            variable = "offset_magnitude", rounding = 2,
                            xlab = "Offset magnitude")



```


```{r}
plot_model_predictions_ocs <- function(offset_dir, hpd_area, model_name, variable, 
                                       range, rounding, xlab) {
  ## This function wraps predict_model_results and plot_simulation_results and plots prediction over simulation.
  ## parameters and assumptions as per underpinning functions.
  plot <- plot_simulation_results_osc(range_to_plot = range, hpd_area = hpd_area,
                                      variable = variable, rounding = rounding,
                                      xlab = xlab)
  
  hpd_for_predict = str_extract(hpd_area, "\\d{2}") ## predict_model_results only takes two-digit numbers as input here (for filtering) - this extracts from HPD area above
  
  predicts_tab <- predict_model_results_osc(offset_dir = offset_dir,
                                            hpd_area = hpd_for_predict, 
                                            model_name = model_name,
                                            variable = variable, 
                                            variable_range = seq(range[1],
                                                                 range[2]))
  predicts_tab <- predicts_tab %>%
    rename("variable_to_plot" = variable, ratio_accurate = 1) ##Dovetail the variable names for the plot at the iteration where interactions start getting explored
  
  plot <- plot +
    geom_line(data = predicts_tab)
  
  plot
}
```


```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           model_name = "act_oc_sc_acc",
                           variable = "offset_magnitude",
                           range = c(0, 50), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           variable = "measurement_error",
                           model_name = "os_cs",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement error")
    
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "curve_uncert",
                           model_name = "oc_sc_ocs",
                           range = c(10, 30), rounding = 2.5,
                           xlab = "Curve uncertainty")
```



Now I can actually visualize all the models.

```{r}
## Pos 68%

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "offset_magnitude",
                           range = c(0, 50), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "measurement_error",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement error")
    
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "curve_uncert",
                           range = c(10, 30), rounding = 2.5,
                           xlab = "Curve uncertainty")
```
For the Positive 68% HPD scenario we have a good prediction on the Offset magnitude, good prediction on the effects of the measurement error, but the prediction for the Cure uncertainty begins to deteriorate for ucnertainties of over 20 14C years.



```{r}
## Pos 95%

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "offset_magnitude",
                           range = c(0, 50), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "measurement_error",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement error")
    
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "curve_uncert",
                           range = c(10, 30), rounding = 2.5,
                           xlab = "Curve uncertainty")
```

For the Positive 95% scenario we have a good prediction on offset magnitude, not as good prediction on the impacts of measurement error, and a somewhat biased prediction on impacts of curve uncertainty.



```{r}
## Neg 68%

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_68",
                           variable = "offset_magnitude",
                           range = c(-50, 0), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_68",
                           variable = "measurement_error",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement error")
    
plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_68",
                           variable = "curve_uncert",
                           range = c(10, 30), rounding = 2.5,
                           xlab = "Curve uncertainty")
```
For the 68% neg scenario we have a good prediction for Offset Magnitude (except when offsets are close to zero), good prediction on measurement uncertainty and a similar issue with prediction being limited as curve uncertainty increases.



```{r}
## Neg 95%

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           variable = "offset_magnitude",
                           range = c(-50, 0), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           variable = "measurement_error",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement error")
    
plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           variable = "curve_uncert",
                           range = c(10, 30), rounding = 2.5,
                           xlab = "Curve uncertainty")
```
The 95% neg scenario fits in a similar way to the 95% pos scenario: good offset fit, poor measurement uncertainty fit, and a biased-but-trailing curve uncertainty fit. 

So the model does well on offset magnitudes, but leaves a little to be desired as far as measurement error and curve uncertainty impacts are concerned. The one particular point of interest is that things go wrong differently for the 95 and 68% HPD areas. There are a few options here. First, I can try getting the models to work - perhaps through experimentiang with different interactions. The second option is to use different model specs for the 95% and 68% HPD areas. This could give useful insights in case of differences between the models. Third option is to go straight to predicting and turning back if the predictions on unknown data have systematic issues. Though what I might begin with is plotting out simpler models to check that the logistic can capture the shapes of the simulated distributions we're dealing with. OK, plan!


## Part 11 Try again agin.

For going back to basics I need to modify a few things in the functions. First of all, I want to see a few more models. This means evolving the visualization functions above to take on input in the form of different models (rather than just hacking down to the one we hoped would have worked). Lets kick off with the model that includes all three potential predictors, but does not involve interactions. The AIC/BIC on it is not as good as on some other models, but it provides a starting point to thinking about the quality of fit. 

```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "offset_magnitude",
                           model_name = "offset_sigma_curve_acc",
                           range = c(0, 50), rounding = 2,
                           xlab = "Offset Magnitude")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "offset_magnitude",
                           model_name = "os_acc",
                           range = c(0, 50), rounding = 2,
                           xlab = "Offset Magnitude")

```
Difference on the regression for the Offset magnitude effect is minimal.

```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "measurement_error",
                           model_name = "offset_sigma_curve_acc",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement Error")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "measurement_error",
                           model_name = "os_acc",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement Error")
```

```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "curve_uncert",
                           model_name = "offset_sigma_curve_acc",
                           range = c(4, 40), rounding = 2,
                           xlab = "Curve uncertainty")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "curve_uncert",
                           model_name = "os_acc",
                           range = c(4, 40), rounding = 2,
                           xlab = "Curve uncertainty")
```

```{r}
### Now for the 95% HPDs
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "offset_magnitude",
                           model_name = "offset_sigma_curve_acc",
                           range = c(0, 50), rounding = 2,
                           xlab = "Offset Magnitude")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "offset_magnitude",
                           model_name = "os_acc",
                           range = c(0, 50), rounding = 2,
                           xlab = "Offset Magnitude")
```

```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "measurement_error",
                           model_name = "offset_sigma_curve_acc",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement Error")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "measurement_error",
                           model_name = "os_acc",
                           range = c(8, 32), rounding = 2,
                           xlab = "Measurement Error")
```
```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "curve_uncert",
                           model_name = "offset_sigma_curve_acc",
                           range = c(4, 40), rounding = 2,
                           xlab = "Curve uncertainty")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "curve_uncert",
                           model_name = "os_acc",
                           range = c(4, 40), rounding = 2,
                           xlab = "Curve uncertainty")
```
So a few observations.

First, whatever the model choice, the 95% measurement error estimate just does not work. While the impact is less in models without the interaction between the Offset Magnitude and Measurement Error, it still suffers from a systematic error. The situation is better for the 68% HPD areas, there, however, the estimates are wrong for the effects of calibration curve uncertainty. These issues doe make sense to a degree. From the perspective of calibration, the uncertainties on the measurement error and curve uncertainty are additive. However, in research practice, there is a major difference, in that we have some control over measurement precision, but little to no control over the calibration curve precision. 

First, lets have a look at how the curve uncertainty is distributed.

```{r}
single_cals_curve_uncert_mod %>%
  ggplot(aes(x = curve_uncert)) +
  geom_histogram(fill = "steelblue", bins = 20) +
  theme_classic()
```
This begins highlighting some potential roots of the problem - the values of curve uncertainty are fairly clustered and variable. This makes me suspect that there might be issues stemming from this. In particular, the clumpy clustering and very few points as we go beyond 25 14C yrs uncertainty implies potential issues kicking in. Lets have a look at how things look through time.

```{r}
single_cals_curve_uncert_mod %>%
  ggplot(aes(x = target_year, y = curve_uncert)) +
  geom_point() +
  theme_classic()
```

## Part 12 Using combined Measurement uncertainty and Curve uncertainty
A different way to approach the problem will be to combine uncertainty from the calibration curve and the measurement error and check how well these work as a predictor. After all, at least in terms of simulating in the radiocarbon space (concentrations of 14C and the like) these should be interchangeable. 

The models using those parameters have similar AIC to other options. In terms of parameter values, we can view the outcomes below:

```{r}
##Get the results out

offset_curve_sigma_comb_params <- readRDS("single_cals_curve_uncert_regr.rds") %>%
  filter(str_detect(model, "comb_acc")) %>%
  mutate(model_results = map(model_results, tidy)) %>%
  unnest(cols = c(model_results))
```

```{r}
## Visualize version without interaction
  offset_curve_sigma_comb_params %>%
  mutate(
    is_pos = if_else(is_pos, "pos_offset", "neg_offset"),
    hpd_area = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
    ) %>%
  ggplot(aes(x = is_pos, y = estimate)) +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_point(stat = "identity") +
  geom_linerange(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error)) +
  facet_grid(rows = vars(term), cols = vars(hpd_area), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    ) +
  labs(
    subtitle = "68.2% HPD"
  )
```







```{r}
##Get the results out

offset_curve_sigma_comb_interact_params <- readRDS("single_cals_curve_uncert_regr.rds") %>%
  filter(str_detect(model, "comb_inter")) %>%
  mutate(model_results = map(model_results, tidy)) %>%
  unnest(cols = c(model_results))
```

```{r}
## Visualize version with interaction
  offset_curve_sigma_comb_interact_params %>%
  mutate(
    is_pos = if_else(is_pos, "pos_offset", "neg_offset"),
    hpd_area = if_else(str_detect(model, "68"), "68.2% HPD", "95.4% HPD")
    ) %>%
  ggplot(aes(x = is_pos, y = estimate)) +
  geom_hline(yintercept = 0, color = "grey80") +
  geom_point(stat = "identity") +
  geom_linerange(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error)) +
  facet_grid(rows = vars(term), cols = vars(hpd_area), scales = "free") +
  theme_bw() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    axis.line = element_line(colour = "grey50", size = 0.5),
    #legend.position = "none",
    strip.background = element_rect(
     color="white", fill="white"
     ),
     text = element_text(family = "Corbel"),
    axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, size = 9)
    ) +
  labs(
    subtitle = "68.2% HPD"
  )
```
Interaction model comes back with interesting results. Big challenge is the huge ucnertainty on the interaction term, also intercepts moved completely.

For funsies lets check the intercepts (I won't be able to biteb into the results viz today anyway)

```{r}
offset_curve_sigma_comb_params %>%
  filter(str_detect(term, "Interce")) %>%
  select(is_pos, model, estimate) %>%
  mutate(acc_zero = 1 / (1 + exp(- estimate)))
```

Now lets check for the interaction model
```{r}
offset_curve_sigma_comb_interact_params %>%
  filter(str_detect(term, "Interce")) %>%
  select(is_pos, model, estimate) %>%
  mutate(acc_zero = 1 / (1 + exp(- estimate)))
```
OK, from a hard physical point of view, the interaction model gives us absurd results for the point case. 

Next step - plot this, with relevant uncertainties. Take it from there. Will also need to prep visualizations, possibly not for the interaction case though :)
```{r}
offset_curve_sigma_comb_params %>%
  filter(str_detect(term, "Interce")) %>%
  select(is_pos, model, estimate, std.error) %>%
  mutate(
    acc_zero = 1 / (1 + exp(- estimate)),
    acc_zero_min = 1 / (1 + exp(- (estimate - 2 * std.error))),
    acc_zero_max = 1 / (1 + exp(- (estimate + 2 * std.error))),
    )
```


The models catch the expected results. Next step will be visualization over the simulation data. Need to re-work the functions, of course :P

```{r}

predict_model_results_osc <- function(offset_dir, hpd_area, model_name,
                                      variable, variable_range) {
  ##Function to simplify prediction generation
  ##Takes on offset_dir and hpd_area, which simplify the results table,
  ##Takes on variable name as string and variable range as a sequence
  ##Assumes offset dir, hpd_area, and variable name provided are all correct
  
  
  offset_curve_sigma_os <- readRDS("single_cals_curve_uncert_regr.rds") %>%
    filter(str_detect(model, model_name)) %>%
    mutate(is_pos = if_else(is_pos, "positive", "negative")) %>% ## Change type for filtering
    filter(str_detect(is_pos, offset_dir) & str_detect(model, hpd_area))
  
  predicted_results <- make_predictions(offset_curve_sigma_os$model_results[[1]],
                                        pred = variable,
                                        pred.values = variable_range)
  
  predicted_results
}
#### Oooohhh... it works without modification... Thank you past me!
```


```{r}
## Function tests
predict_model_results_osc(offset_dir = "pos", hpd_area = "68",
                          model_name = "comb_acc",
                          variable = "offset_magnitude", 
                          variable_range = seq(0, 50))

predict_model_results_osc(offset_dir = "neg", hpd_area = "95", 
                          model_name = "comb_acc",
                          variable = "sigma_curve_uncert", 
                          variable_range = seq(10, 35))



```

```{r}
plot_model_predictions_ocs <- function(offset_dir, hpd_area, model_name, variable, 
                                       range, rounding, xlab) {
  ## This function wraps predict_model_results and plot_simulation_results and plots prediction over simulation.
  ## parameters and assumptions as per underpinning functions.
  plot <- plot_simulation_results_osc(range_to_plot = range, hpd_area = hpd_area,
                                      variable = variable, rounding = rounding,
                                      xlab = xlab)
  
  hpd_for_predict = str_extract(hpd_area, "\\d{2}") ## predict_model_results only takes two-digit numbers as input here (for filtering) - this extracts from HPD area above
  
  predicts_tab <- predict_model_results_osc(offset_dir = offset_dir,
                                            hpd_area = hpd_for_predict, 
                                            model_name = model_name,
                                            variable = variable, 
                                            variable_range = seq(range[1],
                                                                 range[2]))
  predicts_tab <- predicts_tab %>%
    rename("variable_to_plot" = variable, ratio_accurate = 1) ##Dovetail the variable names for the plot at the iteration where interactions start getting explored
  
  plot <- plot +
    geom_line(data = predicts_tab)
  
  plot
}
```


```{r}
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           model_name = "comb_acc",
                           variable = "offset_magnitude",
                           range = c(0, 50), rounding = 2.5,
                           xlab = "Offset magnitude")

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_68",
                           model_name = "comb_acc",
                           variable = "offset_magnitude",
                           range = c(-50, 0), rounding = 2.5,
                           xlab = "Offset magnitude")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           model_name = "comb_acc",
                           variable = "offset_magnitude",
                           range = c(0, 50), rounding = 2.5,
                           xlab = "Offset magnitude")

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           model_name = "comb_acc",
                           variable = "offset_magnitude",
                           range = c(-50, 0), rounding = 2.5,
                           xlab = "Offset magnitude")
  
plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_68",
                           variable = "sigma_curve_uncert",
                           model_name = "comb_acc",
                           range = c(10, 40), rounding = 2,
                           xlab = "Measurement and curve error")

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_68",
                           variable = "sigma_curve_uncert",
                           model_name = "comb_acc",
                           range = c(10, 40), rounding = 2,
                           xlab = "Measurement and curve error")

plot_model_predictions_ocs(offset_dir = "pos", hpd_area = "accuracy_95",
                           variable = "sigma_curve_uncert",
                           model_name = "comb_acc",
                           range = c(10, 40), rounding = 2,
                           xlab = "Measurement and curve error")

plot_model_predictions_ocs(offset_dir = "neg", hpd_area = "accuracy_95",
                           variable = "sigma_curve_uncert",
                           model_name = "comb_acc",
                           range = c(10, 40), rounding = 2,
                           xlab = "Measurement and curve error")
```
Fit less than perfect but improvement on what we had before. The next step will be to convert the classical models into Bayesian models built with Stan and thus get closer to getting the probability distributions we need. Having said that, before I move any further on this, there are a few changes that will happen. Since this got started I had to re-install OxCal (much to my misery) and thus I should, at least in principle, be able to put together large numbers of calibrated dates again. Thus, once we get to model testing, the plan will be to simulate another 20k saples for two test models (checking parameter stability) and another 20-30 k in groups of 1000 simulations of the same scenario to test model predictions. As such, the original split data set can go back into the analysis and the analysis can be re-done.

## Part 13 Modelling with STAN
So we've got ourselves a model that more or less fits. We can still see that the fit is not perfect on the measurement and curve side, but we do have an improvement over what has been before. Now comes the main part - moving from this model specification to predicting accuracy based on offset magnitude and curve-measurement uncertainty. For prediction, I will shift into Bayesian and use STAN. 

### STAN Models for part 13 live here
```{r}
##Read and nest the data
single_cals_w_curve_uncert <- read_csv("single_cals_w_curve_uncert.csv")


```

```{r}
single_cals_curve_for_stan <- single_cals_w_curve_uncert %>%
  pivot_longer(c(accuracy_68, accuracy_95),
               names_to = "hpd_area", values_to = "accuracy") %>%
  select(curve_uncert, measurement_error, offset_magnitude,
         hpd_area, accuracy) %>%
  mutate(sigma_curve_uncert = sqrt((measurement_error^2) + (curve_uncert^2)))
```

```{r}
###Test the STAN functions - yes an ugly way to do it, but too few to justify setting up a nicer loop. 
stan_df <- single_cals_curve_for_stan |>
  filter(str_detect(hpd_area, "68") & offset_magnitude > 0)
  
stan_offset_curve_sigma_comb_68_pos <- stan_glm(
  accuracy ~ offset_magnitude + sigma_curve_uncert,
  data = stan_df, family = binomial(link = 'logit'))

stan_df <- single_cals_curve_for_stan |>
  filter(str_detect(hpd_area, "95") & offset_magnitude > 0)
  
stan_offset_curve_sigma_comb_95_pos <- stan_glm(
  accuracy ~ offset_magnitude + sigma_curve_uncert,
  data = stan_df, family = binomial(link = 'logit'))

stan_df <- single_cals_curve_for_stan |>
  filter(str_detect(hpd_area, "68") & offset_magnitude < 0)
  
stan_offset_curve_sigma_comb_68_neg <- stan_glm(
  accuracy ~ offset_magnitude + sigma_curve_uncert,
  data = stan_df, family = binomial(link = 'logit'))

stan_df <- single_cals_curve_for_stan |>
  filter(str_detect(hpd_area, "95") & offset_magnitude < 0)
  
stan_offset_curve_sigma_comb_95_neg <- stan_glm(
  accuracy ~ offset_magnitude + sigma_curve_uncert,
  data = stan_df, family = binomial(link = 'logit'))


stan_offset_curve_sigma_comb <- list(
  stan_offset_curve_sigma_comb_68_pos,
  stan_offset_curve_sigma_comb_68_neg,
  stan_offset_curve_sigma_comb_95_pos,
  stan_offset_curve_sigma_comb_95_neg
)

```

```{r}
##Lets not keep re-running the models if we don't need to :)
write_rds(stan_offset_curve_sigma_comb, "stan_offset_curve_sigma_comb.rds")
```

## Part 14 review STAN models
